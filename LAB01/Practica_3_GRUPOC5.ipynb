{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cmku494ZkoxN"
      },
      "source": [
        "# Aprendizaje Estadístico y Data Mining\n",
        "\n",
        "## Práctica 3: Principal Component Analysis\n",
        "\n",
        "### Objetivo\n",
        "Existen casos en que las variables no se pueden representar visualmente debido a que necesitaríamos varias dimensiones para ello. Para evitar esto, existe una metodología la cual, un set de datos multidimensional, podemos transformarlo para poder explicar gran parte de la información en 2 o 3 dimensiones. Dicha metodología se conoce con el nombre de Principal Component Analysis (PCA). Vamos a aplicarlo a un set de datos que está colgado en Canvas y vamos a dar una serie de explicaciones de que ocurre.\n",
        "\n",
        "* [Link al dataset.](./data/Pokemon.csv)\n",
        "\n",
        "**Instalación de las librerías necesarias**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "#%pip install -q -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Apartado 1\n",
        "\n",
        "**Enunciado:** Estandarizar los datos utilizando el método *StandardScaler* de la librería scikit-learn para asegurar que las diferencias de rango entre las variables no afecten al procesamiento y análisis de la información. Aplicar esta transformación a las variables relevantes antes de realizar cualquier modelado o visualización.\n",
        "\n",
        "**Solución:** \n",
        "\n",
        "El dataset de Pokemon contiene 21 filas y 8 columnas. Las columnas son: *Pokemon*, *Tipo*, *PS*, *Ataque*, *Defensa*, *Ataque especial*, *Defensa especial* y *Velocidad*. Si analizamos las variables numéricas del dataset, podemos ver que las variables *PS*, *Ataque*, *Defensa*, *Ataque especial*, *Defensa especial* y *Velocidad* tienen un rango de valores diferente. Por ejemplo, la variable *PS* tiene un rango de valores entre 1 y 4, mientras que la variable *Velocidad* tiene un rango de valores entre 1 y 5.\n",
        "\n",
        "En este ejercicio vamos a estandarizar las variables numéricas del dataset de Pokemon. Estandarizar significa que vamos a transformar las variables para que tengan una **media de 0** y una **desviación estándar de 1** usando la función [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html). Esto es útil para que las variables tengan el mismo peso en el análisis y evitar que una variable con un rango muy grande tenga más peso que otra con un rango más pequeño.\n",
        "\n",
        "La fórmula de la estandarización que sigue la función *StandardScaler* es la siguiente:\n",
        "$$ Z = \\frac{x - \\mu}{\\sigma} $$\n",
        "\n",
        "Donde:\n",
        "* $Z$ es el valor estandarizado.\n",
        "* $x$ es el valor original.\n",
        "* $\\mu$ es la media de la variable.\n",
        "* $\\sigma$ es la desviación estándar de la variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.image as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from adjustText import adjust_text\n",
        "from matplotlib.offsetbox import AnnotationBbox, OffsetImage\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "CSV_FILE_PATH = \"data/Pokemon.csv\"\n",
        "\n",
        "\n",
        "def cargar_datos() -> pd.DataFrame:\n",
        "    \"\"\"Carga los datos del archivo CSV_FILE_PATH\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame con los datos del archivo CSV_FILE_PATH\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(CSV_FILE_PATH, sep=\",\")\n",
        "    return df\n",
        "\n",
        "\n",
        "def estandarizar_datos(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Estandariza las columnas numéricas de un DataFrame\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): DataFrame con los datos\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame con las columnas numéricas estandarizadas\n",
        "    \"\"\"\n",
        "    # Seleccionamos las columnas numéricas\n",
        "    columnas_numericas = df.select_dtypes(include=['float64', 'int64']).columns\n",
        "    df_columnas_numericas = df[columnas_numericas]\n",
        "\n",
        "    # Estandarizamos las columnas numéricas\n",
        "    scaler = StandardScaler()\n",
        "    col_num_estadard = scaler.fit_transform(df_columnas_numericas)\n",
        "\n",
        "    return pd.DataFrame(col_num_estadard, columns=columnas_numericas)\n",
        "\n",
        "\n",
        "df = cargar_datos()\n",
        "datos_estandarizados = estandarizar_datos(df)\n",
        "\n",
        "# Mostramos las estadísticas de los datos estandarizados\n",
        "print(\"\\nEstadísticas de los datos estandarizados:\")\n",
        "print(datos_estandarizados.describe().round(2))\n",
        "\n",
        "# Verificamos que la media y la desviación estándar de los datos estandarizados\n",
        "print(\"\\nMedia de cada variable:\")\n",
        "print(datos_estandarizados.mean().round(4))\n",
        "print(\"\\nDesviación estándar de cada variable:\")\n",
        "print(datos_estandarizados.std().round(4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Resultados:**\n",
        "* **Media y desviación estándar**:\n",
        "  * Tras estandarizar las variables numéricas, la media de cada variable es cero o muy cercana a cero (Valores entre $-0.0$ y $0.0$). Esto nos confirma que hemos estandarizado correctamente las variables.\n",
        "  \n",
        "  * La desviación estándar de cada variable es $1.026$ en vez de exactamente $1$. Esto se debe a que la función *StandardScaler* no redondea los valores de la desviación estándar. Sin embargo, la desviación estándar es muy cercana a $1$, lo que nos indica que hemos estandarizado correctamente las variables.\n",
        "\n",
        "* **Valores Estandarizados**:\n",
        "  * *Mínimos y máximos*:\n",
        "    * El valor mínimo más extremo es de $-2.12$ en Ataque, lo que significa que hay un Polémon con una estadística de ataque significativamente menor que la media.\n",
        "    \n",
        "    * El valor máximo más alto es $2.28$ en PS, lo que indica que hay un Poémon con un PS significativamente mayor que la media.\n",
        "  \n",
        "  * *Distribución cuartílica*:\n",
        "    * El 25% de los Pokémons tienen valores estandarizados menores a aproximadamente $-1.0$ en ataque y $-0.75$ en defensa, lo que nos indica que hay Pokémons con valores relativamente bajos en estas categorías.\n",
        "    \n",
        "    * El 50% (la mediana) en la mayoría de las categorías/variables está cerca de $0$, lo que indica que hay un balance entre Pokémons con valores por encima y por debajo de la media.\n",
        "\n",
        "    * El 75% de los Pokémons tienen valores por debajo de $1.0$ en la mayoría de las estadísticas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Apartado 2\n",
        "\n",
        "**Enunciado:** Calcular la matriz de covarianza de los datos estandarizados utilizando el método cov de *numpy* y obtener sus autovalores (*eigenvalues*) y autovectores (*eigenvectors*) aplicando el método linalg.eig. A partir de estos valores, construir un DataFrame que muestre el porcentaje de varianza explicada por cada componente y su acumulado. Interpretar estos resultados para determinar cuántos componentes principales son necesarios para representar los datos iniciales de manera efectiva, analizando la importancia de cada componente en la variabilidad total del conjunto de datos.\n",
        "\n",
        "El análisis de componentes transforma las variables originales en nuevas variables (componentes principales), que son combinaciones lineales de las variables originales y están ordenadas según la cantidad de varianza que explican los datos. Cada componente principal tiene:\n",
        "* Un **autovalor**, que representa cuárianza de los datos orginales representa el componente.\n",
        "* Varios **autovectores**, que representan la dirección de la varianza de los datos originales.\n",
        "* Un **porcentaje de varianza explicada**, que indica que porcentaje dela información total del dataset es capturado por el componente.\n",
        "* Un **porcentaje acumulado de varianza explicada**, que indica el porcentaje de información total capturado por los componentes anteriores (Suma progresiva de la varianza explicada). Ayuda a determinar cuántos componentes son necesarios para representar los datos de manera efectiva.\n",
        "\n",
        "**Solución**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calcular_componentes_principales(datos_estandarizados: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Calcula el autovalor, la varianza explicada y la varianza acumulada de los componentes principales\n",
        "\n",
        "    Args:\n",
        "        datos_estandarizados (pd.DataFrame): DataFrame con los datos estandarizados a analizar\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame con los resultados del análisis de componentes principales\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Caculamos la matriz de covarianza utilizando la\n",
        "    # matriz transpuesta de los datos estandarizados\n",
        "    matriz_covarianza = np.cov(datos_estandarizados.T)\n",
        "\n",
        "    # 2. Calculamos los autovalores y autovectores de la matriz de covarianza\n",
        "    autovalores, autovectores = np.linalg.eig(matriz_covarianza)\n",
        "\n",
        "    # 3. Ordenamos por importancia. Para ello, obtenemos los\n",
        "    # índices que ordenarían los autovalores de mayor a menor\n",
        "    indice = autovalores.argsort()[::-1]\n",
        "\n",
        "    # 4. Ordenamos los autovalores y autovectores según los índices\n",
        "    # que calculamos en el paso anterior\n",
        "    autovalores_ordenados = autovalores[indice]\n",
        "    autovectores_ordenados = autovectores[:, indice]\n",
        "\n",
        "    # 5. Calculamos la varianza explicada y acumulada\n",
        "    varianza_total = np.sum(autovalores_ordenados)\n",
        "    varianza_explicada = autovalores_ordenados / varianza_total\n",
        "    varianza_acumulada = np.cumsum(varianza_explicada)\n",
        "\n",
        "    # 6. Crear DataFrame con resultados\n",
        "    componentes = range(1, len(autovalores_ordenados) + 1)\n",
        "    df_varianza = pd.DataFrame({\n",
        "        'Componente': componentes,\n",
        "        'Autovalor': autovalores_ordenados,\n",
        "        'Varianza_Explicada_%': varianza_explicada * 100,\n",
        "        'Varianza_Acumulada_%': varianza_acumulada * 100\n",
        "    })\n",
        "\n",
        "    return df_varianza.round(4)\n",
        "\n",
        "\n",
        "resultado = calcular_componentes_principales(datos_estandarizados)\n",
        "print(\"\\nAnálisis de Componentes Principales:\")\n",
        "print(resultado)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Resultados:**\n",
        "* El **primer** componente explica el **$41.6413\\%$** de la varianza total:\n",
        "  * Este es el componente más importante y el que contiene la mayor cantidad de información de los datos originales.\n",
        "  * Los Pokémons pueden diferenciarse fuertemente según este eje.\n",
        "\n",
        "* El **segundo** componente explica un **$24.5521\\%$**:\n",
        "  * Al sumar el primer y el segundo componente, se representa el **$66.1934\\%$** de la varianza total, lo que significa que en solo dos dimensiones ya capturamos la mayor parte de la información original.\n",
        "  \n",
        "* Con los **tres primeros** compontes, representamos un **$78.5565\\%$** de la varianza total:\n",
        "  * Si representamos los datos en un espacio tridimensional usando PC1, PC2 y PC3, conservamos casi el **$80\\%$** de la información original.\n",
        "\n",
        "* Para representar más del **$95\\%$** de la información original, necesitaríamos al menos **cinco componentes**:\n",
        "  * El cuarto componente añade un $11.3291\\%$ y el quinto un $7.5145\\%$, lo que sumaria la varianza explicada al $97.4\\%$.\n",
        "  * El sexto componente añade un $2.5999\\%$ adicional, por lo que apenas aporta información nueva.\n",
        "\n",
        "Ya que el dataset es bastante pequeño, podemos representar la mayoría de la información original en solo dos dimensiones utilizando los dos primeros componentes principales. Además, si utilizamos tres componentes, podemos representar casi el $80\\%$ de la información original, lo que nos daría un buen equilibrio entre simplificación y representación de la información."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Apartado 3\n",
        "\n",
        "**Enunciado:** Representar gráficamente los individuos del dataset utilizando los valores de los componentes principales obtenidos. Para ello, reducir la dimensionalidad a dos componentes principales y generar un diagrama de dispersión en 2D. Analizar la distribución de los datos en este nuevo espacio reducido, identificando posibles agrupaciones, patrones o tendencias que permitan extraer conclusiones sobre la estructura de los datos originales.\n",
        "\n",
        "* Referencias:\n",
        "  * [Marcadores de puntos en Matplotlib.](https://matplotlib.org/stable/api/markers_api.html)\n",
        "  * [Colores en Matplotlib.](https://matplotlib.org/stable/gallery/color/named_colors.html)\n",
        "  * [Ajuste del texto.](https://adjusttext.readthedocs.io/en/latest/Examples-for-multiple-subplots.html)\n",
        "\n",
        "**Solución**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualizar_pca(datos_originales: pd.DataFrame, datos_estandarizados: pd.DataFrame) -> tuple:\n",
        "    \"\"\"Visualiza los datos en el espacio de las dos primeras componentes principales\n",
        "\n",
        "    Args:\n",
        "        datos_originales (pd.DataFrame): DataFrame con los datos originales a visualizar\n",
        "        datos_estandarizados (pd.DataFrame): DataFrame con los datos estandarizados\n",
        "\n",
        "    Returns:\n",
        "        tuple: Tupla con un DataFrame con los datos proyectados y la figura generada\n",
        "    \"\"\"\n",
        "    matriz_covarianza = np.cov(datos_estandarizados.T)\n",
        "\n",
        "    autovalores, autovectores = np.linalg.eig(matriz_covarianza)\n",
        "\n",
        "    indice = autovalores.argsort()[::-1]\n",
        "    autovectores_ordenados = autovectores[:, indice]\n",
        "\n",
        "    # Proyectamos los datos en el espacio de las dos primeras componentes principales\n",
        "    datos_espacio_2_dimensiones = np.dot(datos_estandarizados, autovectores_ordenados[:, :2])\n",
        "\n",
        "    # Creamos un DataFrame con los datos proyectados\n",
        "    df_datos_espacio_2_dimensiones = pd.DataFrame(\n",
        "        data=datos_espacio_2_dimensiones,\n",
        "        columns=['Componente Principal 1', 'Componente Principal 2']\n",
        "    )\n",
        "\n",
        "    df_datos_espacio_2_dimensiones['Tipo'] = datos_originales['Tipo']\n",
        "    df_datos_espacio_2_dimensiones['Pokemon'] = datos_originales['Pokémon']\n",
        "\n",
        "    # Estilos para los puntos segun el tipo\n",
        "    estilos = {\n",
        "        'Fuego': {'color': 'firebrick', 'marker': 'media/Fuego.png', 'size': 100},\n",
        "        'Agua': {'color': 'steelblue', 'marker': 'media/Agua.png', 'size': 100},\n",
        "        'Planta': {'color': 'forestgreen', 'marker': 'media/Planta.png', 'size': 100},\n",
        "        'Normal': {'color': 'gray', 'marker': 'media/Sin_Color.png', 'size': 100},\n",
        "        'Eléctrico': {'color': 'orange', 'marker': 'media/Electrico.png', 'size': 100},\n",
        "        'Fantasma': {'color': 'mediumpurple', 'marker': 'media/Fantasma.png', 'size': 100},\n",
        "        'default': {'color': 'black', 'marker': 'media/Sin_Xolor.png', 'size': 100},\n",
        "    }\n",
        "\n",
        "    # Creamos una figura para visualizar los datos en el espacio PCA 2D\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    nombres_Pokemons = []\n",
        "\n",
        "    for tipo in datos_originales['Tipo'].unique():\n",
        "        mascara = df_datos_espacio_2_dimensiones['Tipo'] == tipo\n",
        "        estilo = estilos.get(tipo, estilos['default'])\n",
        "\n",
        "        # Cargamos la imagen\n",
        "        try:\n",
        "            imagen = mpimg.imread(estilo['marker'])\n",
        "            marcador_imagen = OffsetImage(imagen, zoom=0.015)\n",
        "        except:\n",
        "            print(f\"No se pudo cargar la imagen para el tipo {tipo}\")\n",
        "            continue\n",
        "\n",
        "        for x, y in zip(df_datos_espacio_2_dimensiones[mascara]['Componente Principal 1'],\n",
        "                df_datos_espacio_2_dimensiones[mascara]['Componente Principal 2']):\n",
        "            ab = AnnotationBbox(marcador_imagen, (x, y), frameon=False)\n",
        "            plt.gca().add_artist(ab)\n",
        "\n",
        "        plt.scatter(\n",
        "            df_datos_espacio_2_dimensiones[mascara]['Componente Principal 1'], # Coordenada X\n",
        "            df_datos_espacio_2_dimensiones[mascara]['Componente Principal 2'], # Coordenada Y\n",
        "            c = estilo['color'],                                               # Color\n",
        "            s = estilo['size'],                                                # Tamaño\n",
        "            label = tipo,                                                      # Etiqueta\n",
        "            alpha = 0.7                                                        # Transparencia\n",
        "        )\n",
        "\n",
        "        # Collect text annotations\n",
        "        for indice, fila in df_datos_espacio_2_dimensiones[mascara].iterrows():\n",
        "            nombres_Pokemons.append(plt.text(\n",
        "                fila['Componente Principal 1'], # Coordenada X\n",
        "                fila['Componente Principal 2'], # Coordenada Y\n",
        "                fila['Pokemon'],                # Texto\n",
        "                fontsize = 8,                   # Tamaño de fuente\n",
        "                alpha = 0.7                     # Transparencia\n",
        "            ))\n",
        "\n",
        "    # Adjust text positions to avoid overlaps\n",
        "    adjust_text(\n",
        "        nombres_Pokemons,\n",
        "        arrowprops=dict(arrowstyle='-', color='slategray', lw=0.5),\n",
        "        expand=(1.7, 2)\n",
        "    )\n",
        "\n",
        "    plt.title('Pokemons en el Espacio de las Componentes Principales')      # Título\n",
        "    plt.xlabel('Primera Componente Principal')                              # Etiqueta eje X\n",
        "    plt.ylabel('Segunda Componente Principal')                              # Etiqueta eje Y\n",
        "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')                  # Leyenda\n",
        "    plt.tight_layout()                                                      # Ajustar diseño\n",
        "    plt.grid(True, color='whitesmoke', linestyle='-', linewidth=0.5)         # Cuadrícula\n",
        "    plt.gca().axhline(y=0, color='darkgrey', linestyle='--', linewidth=0.5) # Línea eje X\n",
        "    plt.gca().axvline(x=0, color='darkgrey', linestyle='--', linewidth=0.5) # Línea eje Y\n",
        "\n",
        "\n",
        "    return df_datos_espacio_2_dimensiones, plt.gcf()\n",
        "\n",
        "# Uso\n",
        "df_datos_espacio_2_dimensiones, fig = visualizar_pca(df, datos_estandarizados)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Resultados:**\n",
        "\n",
        "La representación en el espacio de los dos primeros componentes principales nos permite visualizar la distribución de los Pokémons en un espacio bidimensional. Algunas observaciones interesantes son:\n",
        "\n",
        "* **Agrupaciones claras por tipo y similitud de atributos**:\n",
        "  * Los Pokémons de tipo *Agua* (Azumarill, Milotic, Starmie, Palkia) parecen formar una tendencia en la parte derecha del gráfico.\n",
        "  \n",
        "  * Los Pokémons de tipo *Planta* (Breloom, Abomasnow, Cherubi, Laefeon) están más dispersos, pero algunos se agrupan en la parte inferior-izquierda.\n",
        "  \n",
        "  * Los Pokémons de tipo *Fantasma* (Gengar, Spiritomb, Jiratina, Dusknoir) están bien diferenciados, situándose en posiciones alejadas del resto.\n",
        "  \n",
        "  * Los Pokémons de tipo *Eléctrico* (Jolteon, Magnezone, Ampharos, Pikachu) se distribuyen en diferentes zonas, pero algunos están cerca, lo que nos indica ciertas similitudes.\n",
        "\n",
        "* **Separación de los Pokemons nos indica diferencias en las estadísticas**:\n",
        "  * Pokémons como Palkia y Arcanine tienen valores altos en el primer componente, lo que nos sugiere que sus estadísticas generales son superiores a las métricas más influyentes en este componente.\n",
        "  \n",
        "  * Cherubi es un outlier en la parte inferior izquierda, lo que nos indica que es un Pokémon con características muy distintas respecto a los demás (Cuenta con stats muy bajas en comparación con el resto).\n",
        "  \n",
        "  * Jolteon y Gengar están en la parte superior  derecha, lo que puede significar que su combinación de Velocidad y Ataque Especial es dominante en el primer y segundo componente.\n",
        "\n",
        "  * Typhlosion tiene un valor muy alto en el segundo componente, esto podría significar que la Velocidad y Ataque Especial son  estadísticas influyentes en el segundo componente.\n",
        "\n",
        "  * Los Pokémons de tipo *Agua* y *Planta* muestran cierta agrupación, lo que podría indicarnos similitudes en sus estadísticas.\n",
        "  \n",
        "  * Los Pokémons de tipo *Fantasma* y *Eléctrico* están muy dispersos, lo que nos sugiere que sus estadísticas son muy variadas y no siguen un patrón claro.\n",
        "\n",
        "* **El primer componente y el segundo parecen representar características ofensivas vs. defensivas**:\n",
        "  * El primer componente parece estar relacionado con estadísticas generales de poder como *PS*, *Ataque* y *Defensa*, ya que los Pokémons más fuertes se encuentran en valores altos de este componente.\n",
        "  \n",
        "  * El segundo componente podría estar representando estadísticas ofensivas como *Ataque especial* y *Velocidad*, lo que explicaría por qué Pokémons como Jolteon y Gengar están en la parte superior derecha."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "we3ec_nmqXTb",
        "JmdyuK49OUIQ",
        "udYC3iGR3mTx",
        "CthJqA6a32-K",
        "_5hAKNxWR_Cp"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Data_mining",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
